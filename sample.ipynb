{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introdution to Machine Learning - Course Project Report\n",
    "\n",
    "Group members:\n",
    "   - Grzegorz Prasek\n",
    "   - Jakub Kindracki\n",
    "   - Mykhailo Shamrai\n",
    "   - Mateusz Mikiciuk\n",
    "   - Ernest Mołczan\n",
    "\n",
    "In this report we will describe our implementation of CNN supposed to classify users allowed to the system and users not allowed (binary classification).\n",
    "\n",
    "## Table of contents:\n",
    "1. Dataset\n",
    "2. Exploratory Data Analysis\n",
    "3. Preparing audio files for generating spectrograms\n",
    "3. Generating spectrograms\n",
    "4. Classifying spectrograms for train, test and validation datasets\n",
    "5. Model\n",
    "6. Training loop\n",
    "7. [EXTRA] **interpretability** - visualizing the behavior and function of individual cnn layers and using if for data exploration\n",
    "8. [EXTRA] **uncertainty** - using monte carlo dropout to estimate classification confidence. Comparing dropout to an ensemble of CNN networks.\n",
    "9. [EXTRA] **parameter space** examining how much individual layers of the network change during training. Investigating their re-initialization robustness."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n"
   ],
   "id": "fbc121e30a2defb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1fc95cd0044d93e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "## 1.1 Introduction to the Dataset\n",
    "The project is based on the DAPS (Device and Produced Speech) dataset, which was specifically designed for speech processing and analysis research. The primary goal of this dataset is to provide high-quality speech recordings that can be utilized in applications such as speech recognition, speaker classification, and acoustic analysis.\n",
    "\n",
    "The DAPS dataset was chosen as the primary data source due to its following characteristics:\n",
    "\n",
    "- **Data Quality**: The recordings are clean and diverse, enabling precise testing of models under both laboratory and simulated conditions.\n",
    "- **Speaker Diversity**: The dataset includes recordings from 20 distinct speakers, divided into two classes:\n",
    "  - **Class 1 (Acceptable individuals)**: Includes recordings from speakers F1, F7, F8, M3, M6, and M8.\n",
    "  - **Class 0 (Unacceptable individuals)**: Includes recordings from the remaining 14 speakers.\n",
    "- **Alignment with Project Requirements**: The dataset provides recordings that can be easily transformed into spectrograms, which are essential for the CNN-based approach employed in this project.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Dataset Characteristics\n",
    "Each recording in the DAPS dataset is available as a `.wav` file and exhibits the following features:\n",
    "\n",
    "- **Standard Sampling Format**: All recordings are sampled at 16 kHz, which is sufficient for most speech processing applications.\n",
    "- **Variety in Recording Lengths**: The recordings vary in duration, necessitating preprocessing to standardize the samples for comparability.\n",
    "- **Natural and Artificial Noise**: The dataset includes samples with varying levels of noise, allowing for robustness testing of the model against disturbances.\n",
    "\n",
    "Additionally, the DAPS dataset was selected due to its accessibility and clear licensing terms, which permit its legal use for educational and research purposes.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Data Preparation\n",
    "To effectively utilize the DAPS dataset in the project, several key data preparation steps were undertaken:\n",
    "\n",
    "### a) Data Cleaning\n",
    "The data cleaning process aimed to remove samples that could negatively impact model performance. The following tasks were performed:\n",
    "\n",
    "- **Duplicate Elimination**: Redundant recordings were removed to prevent overrepresentation of certain samples in the training set.\n",
    "- **Silence Removal**: Segments containing silence were identified and eliminated to improve model efficiency.\n",
    "\n",
    "### b) Data Splitting\n",
    "The dataset was split into three subsets:\n",
    "\n",
    "- **Training Set**: 70% of the data, used for model training.\n",
    "- **Validation Set**: 15% of the data, used for model evaluation during training.\n",
    "- **Test Set**: 15% of the data, used for final model evaluation.\n",
    "\n",
    "The split was performed to ensure no overlap between subsets, preventing data leakage, i.e., no fragments of the same recording were included in both the training and test sets.\n",
    "\n",
    "### c) Data Augmentation\n",
    "To increase data diversity and enhance the model's robustness against noise, the following augmentation techniques were applied:\n",
    "\n",
    "- **Adding Background Noise**: Artificial noise of varying intensities was introduced to simulate real-world acoustic conditions.\n",
    "- **Pitch Shifting**: The pitch of recordings was altered to increase speaker diversity.\n",
    "- **Trimming Recordings**: Samples were cropped to a fixed length to ensure consistency across input data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Exceptional Cases in the Data\n",
    "During data analysis, certain samples were identified as particularly challenging for classification:\n",
    "\n",
    "- **Low-Volume Recordings**: Required signal amplification to enhance quality.\n",
    "- **Samples with Significant Background Noise**: These were leveraged to evaluate the model's noise resistance.\n",
    "- **Acoustically Similar Speakers**: These samples demanded special attention during model training.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 Challenges and Solutions\n",
    "Several challenges were encountered while working with the data, which were addressed as follows:\n",
    "\n",
    "### Class Imbalance\n",
    "- **Problem**: Class 1 was underrepresented, with only six speakers compared to 14 in Class 0.\n",
    "- **Solution**: Data augmentation techniques were used to increase the number of samples for Class 1.\n",
    "\n",
    "### Impact of Noise\n",
    "- **Problem**: High levels of noise in some recordings negatively affected classification performance.\n",
    "- **Solution**: A noise reduction process was applied to the audio files, and augmentation with various noise levels was employed to improve robustness.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Conclusion\n",
    "The prepared and processed dataset provided a solid foundation for training and testing the speech recognition model. The preprocessing steps enabled the identification and resolution of potential issues, such as the heterogeneity in recording quality. The final dataset is diverse, well-balanced, and optimized for use in spectrogram-based models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "509391be9a317db8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "46dcbaaf41406ae3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7119944d52c7f569"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Monte Carlo Dropout for Estimating Classification Confidence: Comparison with CNN Ensembles\n",
    "\n",
    "## 1. Introduction\n",
    "Monte Carlo Dropout (MC Dropout) is a powerful technique used to estimate the confidence of predictions in classification tasks. Unlike traditional deep learning approaches that primarily focus on accuracy, MC Dropout allows for uncertainty estimation by leveraging dropout layers during inference.\n",
    "\n",
    "In this chapter, we detail the application of MC Dropout to evaluate the confidence of a CNN-based classifier. Furthermore, we compare the results with those obtained from an ensemble of CNN networks, a well-known method for uncertainty estimation. Visualizations are provided to illustrate the effect of the number of Monte Carlo samples on classification confidence and variance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Monte Carlo Dropout\n",
    "\n",
    "### a) How It Works\n",
    "Monte Carlo Dropout enables the use of dropout layers during inference by keeping the model in training mode. This introduces randomness into the network’s predictions and allows multiple predictions for the same input. The process involves:\n",
    "1. **Activating Dropout During Inference**: Unlike standard evaluation (`model.eval()`), the model remains in training mode (`model.train()`), ensuring random neuron deactivation.\n",
    "2. **Performing Multiple Predictions**: For each input sample, `n` predictions (Monte Carlo samples) are generated.\n",
    "3. **Calculating Mean and Variance**: The mean prediction provides the final class probability, while the variance indicates uncertainty.\n",
    "\n",
    "### b) Advantages of MC Dropout\n",
    "- **Computational Efficiency**: Uses a single trained model, avoiding the need to maintain multiple networks as in ensembles.\n",
    "- **Simple Implementation**: Only requires activating dropout layers during inference and performing multiple passes.\n",
    "- **Uncertainty Quantification**: Provides insight into model confidence through variance analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Experiment Setup\n",
    "\n",
    "### a) Preparation\n",
    "1. **Model**: A pre-trained CNN with a dropout layer in the fully connected layer (FC1) with a dropout probability of 50% (`p=0.5`).\n",
    "2. **Data**: A test set consisting of 15% of the DAPS dataset, converted into spectrograms.\n",
    "3. **Monte Carlo Sampling**: Predictions were performed for different values of Monte Carlo samples (`n`): 5, 10, 50, and a range from 1 to 30.\n",
    "\n",
    "### b) Procedure\n",
    "1. For each test sample:\n",
    "   - Perform `n` predictions using MC Dropout.\n",
    "   - Compute:\n",
    "     - **Mean Probabilities**: Average class probabilities across samples.\n",
    "     - **Standard Deviation**: Variance of predictions for each class.\n",
    "2. Visualize results for:\n",
    "   - Specific values of `n` (5, 10, 50).\n",
    "   - Full range (`n=1` to `n=30`), with averages computed across batches and Monte Carlo samples.\n",
    "\n",
    "### c) Results\n",
    "- **Stabilization of Mean Probabilities**: As `n` increases, mean probabilities become more stable, reflecting reduced randomness.\n",
    "- **Reduction in Variance**: Larger `n` leads to lower uncertainty, improving confidence in predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Visualization\n",
    "\n",
    "### a) Plots for Specific Monte Carlo Sample Sizes (5, 10, 50)\n",
    "For each `n`, the following is plotted:\n",
    "- **X-axis**: Batch indices from the test set.\n",
    "- **Y-axis**: Mean probabilities for each class with error bars representing standard deviation.\n",
    "\n",
    "### b) Plot for the Full Range of Monte Carlo Samples (1–30)\n",
    "For all `n` values, a plot is generated to show:\n",
    "- Average probabilities and variance across all classes and batches.\n",
    "- **X-axis**: Number of Monte Carlo samples (`n`).\n",
    "- **Y-axis**: Averaged probabilities and variance.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Comparison with CNN Ensembles\n",
    "\n",
    "### a) Ensemble Setup\n",
    "To compare MC Dropout with ensembles, 10 CNNs with independent weight initializations were trained. For each test input:\n",
    "- Ensemble predictions were averaged to calculate mean probabilities and variance.\n",
    "\n",
    "### b) Observations\n",
    "1. **Computational Complexity**: MC Dropout requires only a single model, making it significantly faster than maintaining an ensemble of 10 networks.\n",
    "2. **Stability**: With sufficient Monte Carlo samples (e.g., `n=50`), MC Dropout provides results comparable to ensembles.\n",
    "3. **Applicability**: MC Dropout is practical for scenarios with limited computational resources.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conclusion\n",
    "Monte Carlo Dropout is a practical and efficient method for estimating classification confidence in CNNs. By leveraging randomness in dropout layers, it provides robust confidence estimates while remaining computationally efficient. In comparison with ensembles, MC Dropout achieves similar performance with significantly lower resource requirements, making it suitable for real-world applications where computational constraints exist.\n",
    "\n",
    "---\n",
    "\n",
    "### Python Implementation for Visualization\n"
   ],
   "id": "4278371652791061"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
