{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introdution to Machine Learning - Course Project Report\n",
    "\n",
    "Group members:\n",
    "   - Grzegorz Prasek\n",
    "   - Jakub Kindracki\n",
    "   - Mykhailo Shamrai\n",
    "   - Mateusz Mikiciuk\n",
    "   - Ernest MoÅ‚czan\n",
    "\n",
    "In this report we will describe our implementation of CNN supposed to classify users allowed to the system and users not allowed (binary classification).\n",
    "\n",
    "## Table of contents:\n",
    "1. Dataset\n",
    "2. Exploratory Data Analysis\n",
    "3. Preparing audio files for generating spectrograms\n",
    "3. Generating spectrograms\n",
    "4. Classifying spectrograms for train, test and validation datasets\n",
    "5. Model\n",
    "6. Training loop\n",
    "7. [EXTRA] **interpretability** - visualizing the behavior and function of individual cnn layers and using if for data exploration\n",
    "8. [EXTRA] **uncertainty** - using monte carlo dropout to estimate classification confidence. Comparing dropout to an ensemble of CNN networks.\n",
    "9. [EXTRA] **parameter space** examining how much individual layers of the network change during training. Investigating their re-initialization robustness."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Hello World!\")\n",
   "id": "fbc121e30a2defb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Model\n",
    "\n",
    "In this chapter, we will describe the Convolutional Neural Network (CNN) model used for our project. The model is designed to classify spectrogram images into two classes. Below, we provide an overview of the model architecture and the code implementation.\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "The CNN model consists of the following layers:\n",
    "1. **Convolutional Layers**: Three convolutional layers with ReLU activation and max pooling.\n",
    "2. **Fully Connected Layers**: Two fully connected layers with dropout for regularization.\n",
    "3. **Output Layer**: A final fully connected layer for binary classification.\n",
    "\n",
    "The input to the model is a grayscale image with a size of 224x224 pixels.\n",
    "\n",
    "#### Code Implementation\n",
    "\n",
    "Here is the implementation of the `SpectrogramCNN` model in PyTorch:"
   ],
   "id": "b0f57ce2361e265c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "\n",
    "        # Input is grayscale (1 channel), so input channels = 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max Pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Based on 224x224 input size after 3 pooling layers\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Output layer (for binary classification, num_classes=2)\n",
    "\n",
    "        # Dropout (optional, helps prevent overfitting)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activation and Max Pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Output: (32, 112, 112)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Output: (64, 56, 56)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # Output: (128, 28, 28)\n",
    "\n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = x.view(-1, 128 * 28 * 28)  # Flattening the output of conv layers\n",
    "\n",
    "        # Fully connected layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Output: (batch_size, num_classes)\n",
    "\n",
    "        return x"
   ],
   "id": "c5749b8232c12fd9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This model is designed to process grayscale spectrogram images and classify them into one of two classes. The use of convolutional layers helps in extracting spatial features from the images, while the fully connected layers perform the final classification. Dropout is used to prevent overfitting during training.",
   "id": "dacc17ba827f59ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Training Loop\n",
    "\n",
    "In this chapter, we will explain how the training and evaluation of our CNN model are performed. The training loop is responsible for optimizing the model's parameters, while the evaluation loop assesses the model's performance on the validation set.\n",
    "\n",
    "#### Training Loop\n",
    "\n",
    "The training loop involves the following steps:\n",
    "1. **Model Initialization**: The model, loss function, and optimizer are initialized.\n",
    "2. **Epoch Loop**: The training process runs for a specified number of epochs.\n",
    "3. **Batch Loop**: For each epoch, the model processes the training data in batches.\n",
    "4. **Forward Pass**: The model makes predictions on the input data.\n",
    "5. **Loss Calculation**: The loss between the predictions and the true labels is computed.\n",
    "6. **Backward Pass**: Gradients are calculated, and the model's parameters are updated.\n",
    "7. **Validation**: After each epoch, the model is evaluated on the validation set to monitor its performance.\n",
    "\n",
    "Here is the code implementation of the training loop:\n"
   ],
   "id": "c13a30334d4c4ce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "MODEL_PATH = \"./model.pth\"\n",
    "LAST_MODEL_PATH = \"./last_model.pth\"\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=25, learning_rate=0.001, model_path=MODEL_PATH):\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    # For Mac\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"MPS device not found, using CPU\")\n",
    "\n",
    "    # For Windows\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch + 1}/{num_epochs} at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\")\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU if available\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            # Track loss and accuracy\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Ending epoch {epoch + 1}/{num_epochs} at {time.strftime('%H:%M:%S', time.localtime())}, \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), model_path)  # Save the best model\n",
    "\n",
    "    torch.save(model.state_dict(), LAST_MODEL_PATH)  # Save the last model\n",
    "    print(\"Training complete. Best validation accuracy: {:.4f}\".format(best_val_acc))"
   ],
   "id": "46d8d84cd47c72cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Evaluation Loop\n",
    "\n",
    "The evaluation loop involves the following steps:\n",
    "1. **Model Evaluation Mode**: The model is set to evaluation mode to disable dropout and batch normalization.\n",
    "2. **Batch Loop**: The model processes the validation data in batches.\n",
    "3. **Forward Pass**: The model makes predictions on the input data.\n",
    "4. **Loss Calculation**: The loss between the predictions and the true labels is computed.\n",
    "5. **Accuracy Calculation**: The accuracy of the model's predictions is calculated.\n",
    "6. **F1 Score Calculation**: The F1 score is computed to evaluate the model's performance.\n",
    "\n",
    "Here is the code implementation of the evaluation loop:"
   ],
   "id": "db111112124f1167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Collect predictions and labels for F1 score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_labels, all_predictions, average=\"weighted\")  # Change \"weighted\" if you need macro or micro F1 score\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ],
   "id": "fba89c4fff85f3e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These functions together form the core of the training and evaluation process for our CNN model.\n",
   "id": "42ac695e855d10fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
